{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "822bc18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d87832d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f14d0558d70>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "153cb870",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_FOLDS = 5\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-6\n",
    "DROPOUT_RATE = 0.3\n",
    "PATIENCE = 30\n",
    "SCHEDULER_FACTOR = 0.5\n",
    "SCHEDULER_PATIENCE = 5\n",
    "EPOCHS_PER_PRINT = 1\n",
    "\n",
    "TRAIN_DATA_PATH = 'train.pkl'\n",
    "TEST_DATA_PATH = 'test.pkl'\n",
    "MODEL_PATH = 'best_model.pth'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa9be6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_DATA_PATH, \"rb\") as f:\n",
    "    full_train_data = pickle.load(f)\n",
    "with open(TEST_DATA_PATH, \"rb\") as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d30e435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(full_train_data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "adee2cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomAffine(degrees=8, translate=(0.1, 0.1), scale=(0.95, 1.05), shear=5),\n",
    "    transforms.RandomPerspective(distortion_scale=0.1, p=0.5),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    # transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=0),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b1cd442",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPSDataset(Dataset):\n",
    "    def __init__(self, df, labels_available=True, transform=None, is_train=False):\n",
    "        self.data = df\n",
    "        self.id = np.array(self.data['id'], dtype=np.int64)\n",
    "        self.image1 = np.array(self.data['img1'].tolist(), dtype=np.uint8)\n",
    "        self.image2 = np.array(self.data['img2'].tolist(), dtype=np.uint8)\n",
    "        self.labels_available = labels_available\n",
    "        if labels_available:\n",
    "            self.labels = np.array(self.data['label'], dtype=np.int64)\n",
    "        else:\n",
    "            self.labels = None\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img1_np = self.image1[idx]\n",
    "        img2_np = self.image2[idx]\n",
    "        id_val = self.id[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1_np)\n",
    "            img2 = self.transform(img2_np)\n",
    "        else:\n",
    "            img1 = torch.from_numpy(img1_np.astype(np.float32)).unsqueeze(0) / 255.0\n",
    "            img2 = torch.from_numpy(img2_np.astype(np.float32)).unsqueeze(0) / 255.0\n",
    "\n",
    "        sample = {'img1': img1, 'img2': img2, 'id': id_val}\n",
    "        if self.labels_available:\n",
    "            sample['label'] = self.labels[idx]\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4982b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RPSDataset(train_df, labels_available=True, transform=train_transform, is_train=True)\n",
    "validation_dataset = RPSDataset(test_df, labels_available=True, transform=test_transform, is_train=False)\n",
    "test_dataset = RPSDataset(test_data, labels_available=False, transform=test_transform, is_train=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f35df93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConcatenatedCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConcatenatedCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=2, out_channels=16, kernel_size=3, padding=1)\n",
    "#         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "#         self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.flattend_size = 32 * 6 * 6\n",
    "\n",
    "#         self.fc1 = nn.Linear(self.flattend_size, 64)\n",
    "#         self.Dropout = nn.Dropout(0.5)\n",
    "#         self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "#     def forward(self, x1, x2):\n",
    "#         x = torch.cat((x1, x2), dim=1)\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = self.pool1(x)\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = self.pool2(x)\n",
    "\n",
    "#         x = x.view(-1, self.flattend_size)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.Dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cfaad364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the structure that yields the best results so far 3 block 128 dim\n",
    "class BaseFeatureExtractor(nn.Module):\n",
    "    def __init__(self, output_dim=128):\n",
    "        super(BaseFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flattend_size = 128 * 3 * 3\n",
    "        self.fc = nn.Linear(self.flattend_size, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = x.view(-1, self.flattend_size)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return x\n",
    "    \n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, feature_output_dim=128, dropout=DROPOUT_RATE):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.base_feature_extractor = BaseFeatureExtractor(output_dim=feature_output_dim)\n",
    "        self.classifier_input_dim = feature_output_dim * 2\n",
    "        # self.classifier_input_dim = feature_output_dim\n",
    "        self.fc1 = nn.Linear(self.classifier_input_dim, 64)\n",
    "        self.Dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.base_feature_extractor(input1)\n",
    "        output2 = self.base_feature_extractor(input2)\n",
    "        combined = torch.concatenate((output1, output2), dim=1)\n",
    "        # combined = torch.abs(output1 - output2)\n",
    "        x = self.fc1(combined)\n",
    "        x = F.relu(x)\n",
    "        x = self.Dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6cdb61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        img1 = batch['img1'].to(device)\n",
    "        img2 = batch['img2'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(img1, img2)\n",
    "\n",
    "        loss = criterion(outputs.squeeze(), labels.float().squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        predicted = torch.sign(outputs.squeeze())\n",
    "        predicted[predicted == 0] = -1\n",
    "        correct_predictions += (predicted == labels.float().squeeze()).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    epoch_acc = correct_predictions / total_samples\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            img1 = batch['img1'].to(device)\n",
    "            img2 = batch['img2'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(img1, img2)\n",
    "            loss = criterion(outputs.squeeze(), labels.float().squeeze())\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            predicted = torch.sign(outputs.squeeze())\n",
    "            predicted[predicted == 0] = -1\n",
    "            correct_predictions += (predicted == labels.float().squeeze()).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    epoch_acc = correct_predictions / total_samples\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4478f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, train_loader, validation_loader, criterion, optimizer, \n",
    "                 device, num_epochs=NUM_EPOCHS, model_save_path=MODEL_PATH,\n",
    "                 use_scheduler=True, patience = PATIENCE, scheduler_factor=SCHEDULER_FACTOR, \n",
    "                 scheduler_patience=SCHEDULER_PATIENCE, print_every=EPOCHS_PER_PRINT):\n",
    "    best_accuracy = 0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "\n",
    "    scheduler = None\n",
    "    if use_scheduler:\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor = scheduler_factor, \n",
    "                                                   patience=scheduler_patience)\n",
    "\n",
    "    patience = patience\n",
    "    patience_counter = 0\n",
    "\n",
    "    print(\"Start training ...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate_model(model, validation_loader, criterion, device)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "\n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if use_scheduler:\n",
    "            scheduler.step(val_acc)\n",
    "            \n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Model saved with accuracy: {best_accuracy:.4f}\")\n",
    "            best_model = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {patience_counter} epochs without improvement.\")\n",
    "                break\n",
    "    \n",
    "    print(\"Training complete.\")\n",
    "    print(f\"Best validation accuracy: {best_accuracy:.4f}\")\n",
    "    return best_model, history, best_accuracy, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "12bba3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_training_fold(model, train_loader, validation_loader, cretirion, optimizer, \n",
    "#                       device, num_epochs=NUM_EPOCHS, model_save_path=MODEL_PATH,\n",
    "#                       use_scheduler=True, patience = PATIENCE, scheduler_factor=SCHEDULER_FACTOR, \n",
    "#                       scheduler_patience=SCHEDULER_PATIENCE, print_every=EPOCHS_PER_PRINT):\n",
    "#     best_accuracy = 0\n",
    "#     history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "\n",
    "#     scheduler = None\n",
    "#     if use_scheduler:\n",
    "#         scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor = scheduler_factor, \n",
    "#                                                    patience=scheduler_patience)\n",
    "\n",
    "#     patience = PATIENCE\n",
    "#     patience_counter = 0\n",
    "#     best_model_state = None\n",
    "\n",
    "#     print(\"Start training ...\")\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         train_loss, train_acc = train_model(model, train_loader, cretirion, optimizer, device)\n",
    "#         val_loss, val_acc = validate_model(model, validation_loader, cretirion, device)\n",
    "#         current_lr = optimizer.param_groups[0]['lr']\n",
    "#         history['train_loss'].append(train_loss)\n",
    "#         history['train_acc'].append(train_acc)\n",
    "#         history['val_loss'].append(val_loss)\n",
    "#         history['val_acc'].append(val_acc)\n",
    "#         history['lr'].append(current_lr)\n",
    "\n",
    "#         if (epoch+1) % print_every == 0:\n",
    "#             print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "#                 f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "#         if use_scheduler:\n",
    "#             scheduler.step(val_acc)\n",
    "\n",
    "#         if val_acc > best_accuracy:\n",
    "#             best_accuracy = val_acc\n",
    "#             torch.save(model.state_dict(), model_save_path)\n",
    "#             print(f\"Model saved with accuracy: {best_accuracy:.4f}\")\n",
    "#             best_model_state = model.state_dict()\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "#             if patience_counter >= patience:\n",
    "#                 print(f\"Early stopping triggered after {patience_counter} epochs without improvement.\")\n",
    "#                 break\n",
    "    \n",
    "#     print(\"Training complete.\")\n",
    "#     print(f\"Best validation accuracy: {best_accuracy:.4f}\")\n",
    "#     return model_save_path, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6d232c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
    "# fold_results = []\n",
    "# test_predictions = []\n",
    "\n",
    "# full_rps_dataset = RPSDataset(full_train_data, labels_available=True, transform=None, is_train=True)\n",
    "\n",
    "# test_dataset_obj = RPSDataset(test_data, labels_available=False, transform=test_transform, is_train=False)\n",
    "# test_loader_kf = DataLoader(test_dataset_obj, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(full_train_data)):\n",
    "#     print(f\"Fold {fold+1}/{K_FOLDS}\")\n",
    "#     train_subset = Subset(full_rps_dataset, train_idx)\n",
    "#     val_subset = Subset(full_rps_dataset, val_idx)\n",
    "\n",
    "#     train_dataset_fold = RPSDataset(full_train_data.iloc[train_idx], labels_available=True, transform=train_transform, is_train=True)\n",
    "#     val_dataset_fold = RPSDataset(full_train_data.iloc[val_idx], labels_available=True, transform=test_transform, is_train=False)\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset_fold, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset_fold, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "#     model_fold = SiameseNetwork(dropout=DROPOUT_RATE).to(device)\n",
    "#     criterion_fold = nn.SoftMarginLoss()\n",
    "#     optimizer_fold = optim.AdamW(model_fold.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "#     model_path_fold = f\"best_model_fold_{fold+1}.pth\"\n",
    "\n",
    "#     best_model_path, best_accuracy = run_training_fold(model_fold, train_loader, val_loader, criterion_fold, optimizer_fold,\n",
    "#                                                       device, num_epochs=NUM_EPOCHS, model_save_path=model_path_fold,\n",
    "#                                                       use_scheduler=True, patience=PATIENCE,\n",
    "#                                                       scheduler_factor=SCHEDULER_FACTOR,\n",
    "#                                                       scheduler_patience=SCHEDULER_PATIENCE,\n",
    "#                                                       print_every=EPOCHS_PER_PRINT)\n",
    "    \n",
    "#     fold_results.append({\n",
    "#         'fold': fold + 1,\n",
    "#         'best_accuracy': best_accuracy,\n",
    "#         'model_path': best_model_path\n",
    "#     })\n",
    "\n",
    "#     # Load the best model for inference\n",
    "#     best_model_fold = SiameseNetwork(dropout=DROPOUT_RATE).to(device)\n",
    "#     best_model_fold.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "#     best_model_fold.eval()\n",
    "\n",
    "#     # Inference on the test set\n",
    "#     test_predictions_fold = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_loader_kf:\n",
    "#             img1 = batch['img1'].to(device)\n",
    "#             img2 = batch['img2'].to(device)\n",
    "\n",
    "#             outputs_original = best_model_fold(img1, img2).squeeze()\n",
    "\n",
    "#             img1_hf = TF.hflip(img1)\n",
    "#             img2_hf = TF.hflip(img2)\n",
    "#             outputs_hf = best_model_fold(img1_hf, img2_hf).squeeze()\n",
    "\n",
    "#             if outputs_original.dim() == 0:\n",
    "#                 outputs_original = outputs_original.unsqueeze(0)\n",
    "#             if outputs_hf.dim() == 0:\n",
    "#                 outputs_hf = outputs_hf.unsqueeze(0)\n",
    "\n",
    "#             average_outputs = (outputs_original + outputs_hf) / 2.0\n",
    "#             test_predictions_fold.append(average_outputs.cpu().numpy())\n",
    "\n",
    "#     test_predictions.append(np.concatenate(test_predictions_fold))\n",
    "\n",
    "#     del model_fold, optimizer_fold, train_loader, val_loader, best_model_fold\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93f996c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for res in fold_results:\n",
    "#     print(f\"Fold {res['fold']}: Best Accuracy: {res['best_accuracy']:.4f}, Model Path: {res['model_path']}\")\n",
    "# test_predictions = [np.array(pred) for pred in test_predictions]\n",
    "\n",
    "# ensemble_predictions = np.mean(np.stack(test_predictions, axis=0), axis=0)\n",
    "# print(f\"Shape of ensemble predictions: {ensemble_predictions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57b3cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_ensemble_predictions = np.sign(ensemble_predictions)\n",
    "# final_ensemble_predictions[final_ensemble_predictions == 0] = -1\n",
    "\n",
    "# test_ids_ensemble = test_dataset_obj.id\n",
    "# submission_df = pd.DataFrame({\n",
    "#     'id': test_ids_ensemble,\n",
    "#     'label': final_ensemble_predictions.astype(int)\n",
    "# })\n",
    "# submission_df.to_csv('submission.csv', index=False)\n",
    "# print(\"Submission file created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1aab2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = ConcatenatedCNN().to(device)\n",
    "# model = SiameseNetwork().to(device)\n",
    "# criterion = nn.SoftMarginLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9dc05580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with learning rate: 1e-06, weight decay: 1e-06, dropout rate: 0.3\n",
      "Start training ...\n",
      "Epoch [1/20], Train Loss: 0.2821, Train Acc: 0.8777, Val Loss: 0.3943, Val Acc: 0.8460\n",
      "Model saved with accuracy: 0.8460\n",
      "Epoch [2/20], Train Loss: 0.2787, Train Acc: 0.8809, Val Loss: 0.3952, Val Acc: 0.8468\n",
      "Model saved with accuracy: 0.8468\n",
      "Epoch [3/20], Train Loss: 0.2816, Train Acc: 0.8791, Val Loss: 0.3960, Val Acc: 0.8465\n",
      "Epoch [4/20], Train Loss: 0.2761, Train Acc: 0.8824, Val Loss: 0.3968, Val Acc: 0.8468\n",
      "Epoch [5/20], Train Loss: 0.2788, Train Acc: 0.8818, Val Loss: 0.3975, Val Acc: 0.8460\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n\u001b[32m     17\u001b[39m model_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbest_model_lr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_wd_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwd\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_dr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m final_model, history, best_acc, last_model = \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m                                              \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m results.append({\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m: lr, \u001b[33m'\u001b[39m\u001b[33mwd\u001b[39m\u001b[33m'\u001b[39m: wd, \u001b[33m'\u001b[39m\u001b[33mdr\u001b[39m\u001b[33m'\u001b[39m: dr, \u001b[33m'\u001b[39m\u001b[33mbest_val_acc\u001b[39m\u001b[33m'\u001b[39m: best_acc, \u001b[33m'\u001b[39m\u001b[33mhistory\u001b[39m\u001b[33m'\u001b[39m: history})\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest validation accuracy for lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, wd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwd\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, dr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m(model, train_loader, validation_loader, criterion, optimizer, device, num_epochs, model_save_path, use_scheduler, patience, scheduler_factor, scheduler_patience, print_every)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStart training ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     train_loss, train_acc = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     val_loss, val_acc = validate_model(model, validation_loader, criterion, device)\n\u001b[32m     21\u001b[39m     current_lr = optimizer.param_groups[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, device)\u001b[39m\n\u001b[32m      4\u001b[39m correct_predictions = \u001b[32m0\u001b[39m\n\u001b[32m      5\u001b[39m total_samples = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimg1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimg2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Kaggle/tcenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Kaggle/tcenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Kaggle/tcenv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mRPSDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     21\u001b[39m id_val = \u001b[38;5;28mself\u001b[39m.id[idx]\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     img1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     img2 = \u001b[38;5;28mself\u001b[39m.transform(img2_np)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Kaggle/tcenv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Kaggle/tcenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Kaggle/tcenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Kaggle/tcenv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:277\u001b[39m, in \u001b[36mNormalize.forward\u001b[39m\u001b[34m(self, tensor)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) -> Tensor:\n\u001b[32m    270\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[33;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m \u001b[33;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Kaggle/tcenv/lib/python3.12/site-packages/torchvision/transforms/functional.py:346\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Normalize a float tensor image with mean and standard deviation.\u001b[39;00m\n\u001b[32m    329\u001b[39m \u001b[33;03mThis transform does not support PIL Image.\u001b[39;00m\n\u001b[32m    330\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    343\u001b[39m \u001b[33;03m    Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[32m    344\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_tracing():\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[43m_log_api_usage_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch.Tensor):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Kaggle/tcenv/lib/python3.12/site-packages/torchvision/utils.py:646\u001b[39m, in \u001b[36m_log_api_usage_once\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m    629\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    630\u001b[39m \u001b[33;03mLogs API usage(module and name) within an organization.\u001b[39;00m\n\u001b[32m    631\u001b[39m \u001b[33;03mIn a large ecosystem, it's often useful to track the PyTorch and\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    643\u001b[39m \u001b[33;03m    obj (class instance or method): an object to extract info from.\u001b[39;00m\n\u001b[32m    644\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    645\u001b[39m module = obj.\u001b[34m__module__\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtorchvision\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m    647\u001b[39m     module = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtorchvision.internal.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    648\u001b[39m name = obj.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "learning_rates = [1e-6]\n",
    "weight_decays = [1e-6]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dropout_rates = [0.3]\n",
    "\n",
    "parameter_combinations = list(itertools.product(learning_rates, weight_decays, dropout_rates))\n",
    "results = []\n",
    "\n",
    "for lr, wd, dr in parameter_combinations:\n",
    "    print(f\"Training with learning rate: {lr}, weight decay: {wd}, dropout rate: {dr}\")\n",
    "    model = SiameseNetwork(dropout=dr).to(device)\n",
    "    model.load_state_dict(torch.load(\"all_best_model/3block_128dim_lr_1e-05_wd_1e-05_dr_0.5.pth\"))\n",
    "    criterion = nn.SoftMarginLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    model_path = f\"best_model_lr_{lr}_wd_{wd}_dr_{dr}.pth\"\n",
    "\n",
    "    final_model, history, best_acc, last_model = run_training(model, train_loader, validation_loader, criterion, optimizer, \n",
    "                                                  device, NUM_EPOCHS, model_path, use_scheduler=False, patience=NUM_EPOCHS)\n",
    "    \n",
    "    results.append({'lr': lr, 'wd': wd, 'dr': dr, 'best_val_acc': best_acc, 'history': history})\n",
    "    print(f\"Best validation accuracy for lr={lr}, wd={wd}, dr={dr}: {best_acc:.4f}\")\n",
    "\n",
    "    del model\n",
    "    del optimizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c851d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_1 = SiameseNetwork(dropout=0.3).to(device)\n",
    "# best_model_1.load_state_dict(torch.load(\"3block_128dim_lr_5e-05_wd_1e-06_dr_0.3.pth\"))\n",
    "# best_model_1.eval()\n",
    "# predictions = []\n",
    "# test_ids = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_loader:\n",
    "#         img1 = batch['img1'].to(device)\n",
    "#         img2 = batch['img2'].to(device)\n",
    "#         ids = batch['id']\n",
    "\n",
    "#         outputs = best_model_1(img1, img2)\n",
    "#         predicted = torch.sign(outputs.squeeze())\n",
    "#         predicted[predicted == 0] = -1\n",
    "\n",
    "#         predicted = predicted.cpu().numpy()\n",
    "#         ids = ids.numpy()\n",
    "\n",
    "#         predictions.extend(predicted)\n",
    "#         if isinstance(ids, torch.Tensor):\n",
    "#             test_ids.extend(ids.cpu().numpy().tolist())\n",
    "#         else:\n",
    "#             test_ids.extend(ids)\n",
    "\n",
    "# submission_df = pd.DataFrame({'id': test_ids, 'label': predictions})\n",
    "# submission_df['label'] = submission_df['label'].astype(int)\n",
    "# submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4562e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_1 = SiameseNetwork(dropout=0.3).to(device)\n",
    "best_model_1.load_state_dict(torch.load(\"best_model_lr_1e-05_wd_1e-05_dr_0.5.pth\"))\n",
    "best_model_1.eval()\n",
    "tta_predictions = []\n",
    "test_ids_tta = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        img1 = batch['img1'].to(device)\n",
    "        img2 = batch['img2'].to(device)\n",
    "        ids = batch['id']\n",
    "\n",
    "        output_original = best_model_1(img1, img2).squeeze()\n",
    "\n",
    "        img1_hf = TF.hflip(img1)\n",
    "        img2_hf = TF.hflip(img2)\n",
    "        output_hf = best_model_1(img1_hf, img2_hf).squeeze()\n",
    "\n",
    "        if output_original.dim() == 0:\n",
    "            output_original = output_original.unsqueeze(0)\n",
    "        if output_hf.dim() == 0:\n",
    "            output_hf = output_hf.unsqueeze(0)\n",
    "\n",
    "        average_output = (output_original + output_hf) / 2.0\n",
    "\n",
    "        tta_predictions.extend(average_output.cpu().numpy())\n",
    "\n",
    "        if isinstance(ids, torch.Tensor):\n",
    "            test_ids_tta.extend(ids.cpu().numpy().tolist())\n",
    "        else:\n",
    "            test_ids_tta.extend(ids)\n",
    "\n",
    "final_predictions_tta = np.sign(np.array(tta_predictions))\n",
    "final_predictions_tta[final_predictions_tta == 0] = -1\n",
    "\n",
    "submission_df = pd.DataFrame({'id': test_ids_tta, 'label': final_predictions_tta.astype(int)})\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250740e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.409684</td>\n",
       "      <td>0.820611</td>\n",
       "      <td>0.422744</td>\n",
       "      <td>0.84425</td>\n",
       "      <td>1.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.387444</td>\n",
       "      <td>0.823306</td>\n",
       "      <td>0.409138</td>\n",
       "      <td>0.84250</td>\n",
       "      <td>1.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.384957</td>\n",
       "      <td>0.823083</td>\n",
       "      <td>0.399084</td>\n",
       "      <td>0.84225</td>\n",
       "      <td>1.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.378467</td>\n",
       "      <td>0.826750</td>\n",
       "      <td>0.395369</td>\n",
       "      <td>0.84175</td>\n",
       "      <td>1.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.375652</td>\n",
       "      <td>0.828528</td>\n",
       "      <td>0.394458</td>\n",
       "      <td>0.84125</td>\n",
       "      <td>1.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.373304</td>\n",
       "      <td>0.831472</td>\n",
       "      <td>0.390618</td>\n",
       "      <td>0.84275</td>\n",
       "      <td>1.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.370057</td>\n",
       "      <td>0.829639</td>\n",
       "      <td>0.392190</td>\n",
       "      <td>0.84425</td>\n",
       "      <td>1.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.370574</td>\n",
       "      <td>0.832889</td>\n",
       "      <td>0.391002</td>\n",
       "      <td>0.84350</td>\n",
       "      <td>5.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.370326</td>\n",
       "      <td>0.830333</td>\n",
       "      <td>0.390883</td>\n",
       "      <td>0.83975</td>\n",
       "      <td>5.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.370434</td>\n",
       "      <td>0.832806</td>\n",
       "      <td>0.391502</td>\n",
       "      <td>0.84075</td>\n",
       "      <td>5.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.368198</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.391798</td>\n",
       "      <td>0.84375</td>\n",
       "      <td>5.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.369464</td>\n",
       "      <td>0.830472</td>\n",
       "      <td>0.390988</td>\n",
       "      <td>0.84225</td>\n",
       "      <td>5.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.373930</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.390594</td>\n",
       "      <td>0.84425</td>\n",
       "      <td>5.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.371106</td>\n",
       "      <td>0.830444</td>\n",
       "      <td>0.390618</td>\n",
       "      <td>0.84375</td>\n",
       "      <td>2.500000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.366788</td>\n",
       "      <td>0.836361</td>\n",
       "      <td>0.391056</td>\n",
       "      <td>0.84325</td>\n",
       "      <td>2.500000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.364851</td>\n",
       "      <td>0.831806</td>\n",
       "      <td>0.390902</td>\n",
       "      <td>0.84400</td>\n",
       "      <td>2.500000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.369500</td>\n",
       "      <td>0.830389</td>\n",
       "      <td>0.391052</td>\n",
       "      <td>0.84375</td>\n",
       "      <td>2.500000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.371326</td>\n",
       "      <td>0.830500</td>\n",
       "      <td>0.390669</td>\n",
       "      <td>0.84400</td>\n",
       "      <td>2.500000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.366277</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.390537</td>\n",
       "      <td>0.84300</td>\n",
       "      <td>2.500000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.363181</td>\n",
       "      <td>0.834222</td>\n",
       "      <td>0.390830</td>\n",
       "      <td>0.84350</td>\n",
       "      <td>1.250000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.362689</td>\n",
       "      <td>0.833389</td>\n",
       "      <td>0.390644</td>\n",
       "      <td>0.84350</td>\n",
       "      <td>1.250000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.362841</td>\n",
       "      <td>0.836278</td>\n",
       "      <td>0.390906</td>\n",
       "      <td>0.84375</td>\n",
       "      <td>1.250000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.364014</td>\n",
       "      <td>0.833500</td>\n",
       "      <td>0.391257</td>\n",
       "      <td>0.84300</td>\n",
       "      <td>1.250000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.358513</td>\n",
       "      <td>0.835778</td>\n",
       "      <td>0.391541</td>\n",
       "      <td>0.84300</td>\n",
       "      <td>1.250000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.365783</td>\n",
       "      <td>0.834167</td>\n",
       "      <td>0.391602</td>\n",
       "      <td>0.84275</td>\n",
       "      <td>1.250000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.371850</td>\n",
       "      <td>0.830750</td>\n",
       "      <td>0.391292</td>\n",
       "      <td>0.84325</td>\n",
       "      <td>6.250000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.366528</td>\n",
       "      <td>0.832639</td>\n",
       "      <td>0.391280</td>\n",
       "      <td>0.84300</td>\n",
       "      <td>6.250000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.367051</td>\n",
       "      <td>0.834111</td>\n",
       "      <td>0.391206</td>\n",
       "      <td>0.84275</td>\n",
       "      <td>6.250000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.367474</td>\n",
       "      <td>0.831444</td>\n",
       "      <td>0.391338</td>\n",
       "      <td>0.84225</td>\n",
       "      <td>6.250000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.362134</td>\n",
       "      <td>0.834306</td>\n",
       "      <td>0.391387</td>\n",
       "      <td>0.84225</td>\n",
       "      <td>6.250000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.364442</td>\n",
       "      <td>0.835306</td>\n",
       "      <td>0.391275</td>\n",
       "      <td>0.84275</td>\n",
       "      <td>6.250000e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_loss  train_acc  val_loss  val_acc            lr\n",
       "0     0.409684   0.820611  0.422744  0.84425  1.000000e-05\n",
       "1     0.387444   0.823306  0.409138  0.84250  1.000000e-05\n",
       "2     0.384957   0.823083  0.399084  0.84225  1.000000e-05\n",
       "3     0.378467   0.826750  0.395369  0.84175  1.000000e-05\n",
       "4     0.375652   0.828528  0.394458  0.84125  1.000000e-05\n",
       "5     0.373304   0.831472  0.390618  0.84275  1.000000e-05\n",
       "6     0.370057   0.829639  0.392190  0.84425  1.000000e-05\n",
       "7     0.370574   0.832889  0.391002  0.84350  5.000000e-06\n",
       "8     0.370326   0.830333  0.390883  0.83975  5.000000e-06\n",
       "9     0.370434   0.832806  0.391502  0.84075  5.000000e-06\n",
       "10    0.368198   0.832000  0.391798  0.84375  5.000000e-06\n",
       "11    0.369464   0.830472  0.390988  0.84225  5.000000e-06\n",
       "12    0.373930   0.827556  0.390594  0.84425  5.000000e-06\n",
       "13    0.371106   0.830444  0.390618  0.84375  2.500000e-06\n",
       "14    0.366788   0.836361  0.391056  0.84325  2.500000e-06\n",
       "15    0.364851   0.831806  0.390902  0.84400  2.500000e-06\n",
       "16    0.369500   0.830389  0.391052  0.84375  2.500000e-06\n",
       "17    0.371326   0.830500  0.390669  0.84400  2.500000e-06\n",
       "18    0.366277   0.835000  0.390537  0.84300  2.500000e-06\n",
       "19    0.363181   0.834222  0.390830  0.84350  1.250000e-06\n",
       "20    0.362689   0.833389  0.390644  0.84350  1.250000e-06\n",
       "21    0.362841   0.836278  0.390906  0.84375  1.250000e-06\n",
       "22    0.364014   0.833500  0.391257  0.84300  1.250000e-06\n",
       "23    0.358513   0.835778  0.391541  0.84300  1.250000e-06\n",
       "24    0.365783   0.834167  0.391602  0.84275  1.250000e-06\n",
       "25    0.371850   0.830750  0.391292  0.84325  6.250000e-07\n",
       "26    0.366528   0.832639  0.391280  0.84300  6.250000e-07\n",
       "27    0.367051   0.834111  0.391206  0.84275  6.250000e-07\n",
       "28    0.367474   0.831444  0.391338  0.84225  6.250000e-07\n",
       "29    0.362134   0.834306  0.391387  0.84225  6.250000e-07\n",
       "30    0.364442   0.835306  0.391275  0.84275  6.250000e-07"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "pd.DataFrame(results_df['history'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e91a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "test_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        img1 = batch['img1'].to(device)\n",
    "        img2 = batch['img2'].to(device)\n",
    "        ids = batch['id']\n",
    "\n",
    "        outputs = model(img1, img2)\n",
    "        predicted = torch.sign(outputs.squeeze())\n",
    "        predicted[predicted == 0] = -1\n",
    "\n",
    "        predicted = predicted.cpu().numpy()\n",
    "        ids = ids.numpy()\n",
    "\n",
    "        predictions.extend(predicted)\n",
    "        if isinstance(ids, torch.Tensor):\n",
    "            test_ids.extend(ids.cpu().numpy().tolist())\n",
    "        else:\n",
    "            test_ids.extend(ids)\n",
    "\n",
    "submission_df = pd.DataFrame({'id': test_ids, 'label': predictions})\n",
    "submission_df['label'] = submission_df['label'].astype(int)\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created: submission.csv\")\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"siamese_model.pth\")\n",
    "print(\"Model saved as siamese_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
