{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b88b02a6",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822bc18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d5108",
   "metadata": {},
   "source": [
    "## Configuratioin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87832d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153cb870",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_FOLDS = 5\n",
    "NUM_EPOCHS = 1000\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 1\n",
    "DROPOUT_RATE = 0.3\n",
    "PATIENCE = 30\n",
    "SCHEDULER_FACTOR = 0.1\n",
    "SCHEDULER_PATIENCE = 15\n",
    "EPOCHS_PER_PRINT = 1\n",
    "\n",
    "TRAIN_DATA_PATH = 'train.pkl'\n",
    "TEST_DATA_PATH = 'test.pkl'\n",
    "MODEL_PATH = 'best_model.pth'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef579fe",
   "metadata": {},
   "source": [
    "## Load datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9be6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_DATA_PATH, \"rb\") as f:\n",
    "    full_train_data = pickle.load(f)\n",
    "with open(TEST_DATA_PATH, \"rb\") as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30e435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(full_train_data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0374cb66",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adee2cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add random flips, rotations, jitters, etc. to the training images.\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomAffine(degrees=8, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=5),\n",
    "    # transforms.RandomPerspective(distortion_scale=0.1, p=0.5),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    # transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=0),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1cd442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the dataset\n",
    "class RPSDataset(Dataset):\n",
    "    def __init__(self, df, labels_available=True, transform=None, is_train=False):\n",
    "        self.data = df\n",
    "        self.id = np.array(self.data['id'], dtype=np.int64)\n",
    "        self.image1 = np.array(self.data['img1'].tolist(), dtype=np.uint8)\n",
    "        self.image2 = np.array(self.data['img2'].tolist(), dtype=np.uint8)\n",
    "        self.labels_available = labels_available\n",
    "        if labels_available:\n",
    "            self.labels = np.array(self.data['label'], dtype=np.int64)\n",
    "        else:\n",
    "            self.labels = None\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img1_np = self.image1[idx]\n",
    "        img2_np = self.image2[idx]\n",
    "        id_val = self.id[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1_np)\n",
    "            img2 = self.transform(img2_np)\n",
    "        else:\n",
    "            img1 = torch.from_numpy(img1_np.astype(np.float32)).unsqueeze(0) / 255.0\n",
    "            img2 = torch.from_numpy(img2_np.astype(np.float32)).unsqueeze(0) / 255.0\n",
    "\n",
    "        sample = {'img1': img1, 'img2': img2, 'id': id_val}\n",
    "        if self.labels_available:\n",
    "            sample['label'] = self.labels[idx]\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RPSDataset(train_df, labels_available=True, transform=train_transform, is_train=True)\n",
    "validation_dataset = RPSDataset(test_df, labels_available=True, transform=test_transform, is_train=False)\n",
    "test_dataset = RPSDataset(test_data, labels_available=False, transform=test_transform, is_train=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ae010e",
   "metadata": {},
   "source": [
    "## Set up the model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the squeeze-excitation block\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(batch_size, channels)\n",
    "        y = self.fc(y).view(batch_size, channels, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, use_se=True):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.use_se = use_se\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        if use_se:\n",
    "            self.se_block = SEBlock(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.use_se:\n",
    "            out = self.se_block(out)\n",
    "\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaad364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the base feature extractor\n",
    "class BaseFeatureExtractor(nn.Module):\n",
    "    def __init__(self, output_dim=256, use_se=True):\n",
    "        super(BaseFeatureExtractor, self).__init__()\n",
    "        self.in_channels = 32\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layer1 = self._make_layer(ResidualBlock, 64, num_blocks=2, stride=2, use_se=use_se)\n",
    "\n",
    "        self.layer2 = self._make_layer(ResidualBlock, 128, num_blocks=2, stride=2, use_se=use_se)\n",
    "\n",
    "        self.layer3 = self._make_layer(ResidualBlock, 256, num_blocks=2, stride=2, use_se=use_se)\n",
    "\n",
    "        self.layer4 = self._make_layer(ResidualBlock, 512, num_blocks=2, stride=1, use_se=use_se)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, output_dim)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride=1, use_se=True):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride, use_se))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Siamese network\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, feature_output_dim=256, dropout=DROPOUT_RATE, use_se=True, interaction='concat'):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.base_feature_extractor = BaseFeatureExtractor(output_dim=feature_output_dim, use_se=use_se)\n",
    "        self.interaction = interaction\n",
    "\n",
    "        if interaction == 'concat':\n",
    "            self.classifier_input_dim = feature_output_dim * 2\n",
    "        elif interaction == 'abs':\n",
    "            self.classifier_input_dim = feature_output_dim\n",
    "        elif interaction == 'both':\n",
    "            self.classifier_input_dim = feature_output_dim * 3\n",
    "        else:\n",
    "            raise ValueError(\"Invalid interaction type.\")\n",
    "\n",
    "        self.fc1 = nn.Linear(self.classifier_input_dim, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.Dropout1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.Dropout2 = nn.Dropout(dropout / 2)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.base_feature_extractor(input1)\n",
    "        output2 = self.base_feature_extractor(input2)\n",
    "\n",
    "        if self.interaction == 'concat':\n",
    "            combined = torch.cat((output1, output2), dim=1)\n",
    "        elif self.interaction == 'abs':\n",
    "            combined = torch.abs(output1 - output2)\n",
    "        elif self.interaction == 'both':\n",
    "            combined = torch.cat((output1, output2, torch.abs(output1 - output2)), dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid interaction type.\")\n",
    "\n",
    "        x = self.fc1(combined)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.Dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.Dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac636d4",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        img1 = batch['img1'].to(device)\n",
    "        img2 = batch['img2'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(img1, img2)\n",
    "\n",
    "        loss = criterion(outputs.squeeze(), labels.float().squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        predicted = torch.sign(outputs.squeeze())\n",
    "        predicted[predicted == 0] = -1\n",
    "        correct_predictions += (predicted == labels.float().squeeze()).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    epoch_acc = correct_predictions / total_samples\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            img1 = batch['img1'].to(device)\n",
    "            img2 = batch['img2'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(img1, img2)\n",
    "            loss = criterion(outputs.squeeze(), labels.float().squeeze())\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            predicted = torch.sign(outputs.squeeze())\n",
    "            predicted[predicted == 0] = -1\n",
    "            correct_predictions += (predicted == labels.float().squeeze()).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    epoch_acc = correct_predictions / total_samples\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4478f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, train_loader, validation_loader, criterion, optimizer, \n",
    "                 device, num_epochs=NUM_EPOCHS, model_save_path=MODEL_PATH,\n",
    "                 use_scheduler=True, patience = PATIENCE, scheduler_factor=SCHEDULER_FACTOR, \n",
    "                 scheduler_patience=SCHEDULER_PATIENCE, print_every=EPOCHS_PER_PRINT):\n",
    "    best_accuracy = 0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "\n",
    "    scheduler = None\n",
    "    if use_scheduler:\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor = scheduler_factor, \n",
    "                                                   patience=scheduler_patience)\n",
    "\n",
    "    patience = patience\n",
    "    patience_counter = 0\n",
    "\n",
    "    print(\"Start training ...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate_model(model, validation_loader, criterion, device)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "\n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if use_scheduler:\n",
    "            scheduler.step(val_acc)\n",
    "            \n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Model saved with accuracy: {best_accuracy:.4f}\")\n",
    "            best_model = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {patience_counter} epochs without improvement.\")\n",
    "                break\n",
    "    \n",
    "    print(\"Training complete.\")\n",
    "    print(f\"Best validation accuracy: {best_accuracy:.4f}\")\n",
    "    return best_model, history, best_accuracy, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b142fe75",
   "metadata": {},
   "source": [
    "Run the block below to see the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc05580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# learning_rates = [1e-6]\n",
    "# weight_decays = [1e-6]\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# dropout_rates = [0.3]\n",
    "\n",
    "# parameter_combinations = list(itertools.product(learning_rates, weight_decays, dropout_rates))\n",
    "# results = []\n",
    "\n",
    "# for lr, wd, dr in parameter_combinations:\n",
    "#     print(f\"Training with learning rate: {lr}, weight decay: {wd}, dropout rate: {dr}\")\n",
    "#     model = SiameseNetwork(dropout=dr).to(device)\n",
    "#     model.load_state_dict(torch.load(\"all_best_model/residual_4layers_lr_5e-05_wd_1_dr_0.3.pth\")) # Comment this line to train from scratch\n",
    "#     criterion = nn.SoftMarginLoss()\n",
    "#     optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "#     model_path = f\"residual_4layers_lr_{lr}_wd_{wd}_dr_{dr}.pth\"\n",
    "\n",
    "#     final_model, history, best_acc, last_model = run_training(model, train_loader, validation_loader, criterion, optimizer, \n",
    "#                                                   device, NUM_EPOCHS, model_path, use_scheduler=True, patience=PATIENCE)\n",
    "    \n",
    "#     results.append({'lr': lr, 'wd': wd, 'dr': dr, 'best_val_acc': best_acc, 'history': history})\n",
    "#     print(f\"Best validation accuracy for lr={lr}, wd={wd}, dr={dr}: {best_acc:.4f}\")\n",
    "\n",
    "#     del model\n",
    "#     del optimizer\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5843c5",
   "metadata": {},
   "source": [
    "## Generate predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4562e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_1 = SiameseNetwork(dropout=0.3).to(device)\n",
    "best_model_1.load_state_dict(torch.load(\"residual_4layers_lr_5e-05_wd_1_dr_0.3.pth\"))\n",
    "best_model_1.eval()\n",
    "tta_predictions = []\n",
    "test_ids_tta = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        img1 = batch['img1'].to(device)\n",
    "        img2 = batch['img2'].to(device)\n",
    "        ids = batch['id']\n",
    "\n",
    "        output_original = best_model_1(img1, img2).squeeze()\n",
    "\n",
    "        img1_hf = TF.hflip(img1)\n",
    "        img2_hf = TF.hflip(img2)\n",
    "        output_hf = best_model_1(img1_hf, img2_hf).squeeze()\n",
    "\n",
    "        if output_original.dim() == 0:\n",
    "            output_original = output_original.unsqueeze(0)\n",
    "        if output_hf.dim() == 0:\n",
    "            output_hf = output_hf.unsqueeze(0)\n",
    "\n",
    "        average_output = (output_original + output_hf) / 2.0\n",
    "\n",
    "        tta_predictions.extend(average_output.cpu().numpy())\n",
    "\n",
    "        if isinstance(ids, torch.Tensor):\n",
    "            test_ids_tta.extend(ids.cpu().numpy().tolist())\n",
    "        else:\n",
    "            test_ids_tta.extend(ids)\n",
    "\n",
    "final_predictions_tta = np.sign(np.array(tta_predictions))\n",
    "final_predictions_tta[final_predictions_tta == 0] = -1\n",
    "\n",
    "submission_df = pd.DataFrame({'id': test_ids_tta, 'label': final_predictions_tta.astype(int)})\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e91a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# predictions = []\n",
    "# test_ids = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_loader:\n",
    "#         img1 = batch['img1'].to(device)\n",
    "#         img2 = batch['img2'].to(device)\n",
    "#         ids = batch['id']\n",
    "\n",
    "#         outputs = model(img1, img2)\n",
    "#         predicted = torch.sign(outputs.squeeze())\n",
    "#         predicted[predicted == 0] = -1\n",
    "\n",
    "#         predicted = predicted.cpu().numpy()\n",
    "#         ids = ids.numpy()\n",
    "\n",
    "#         predictions.extend(predicted)\n",
    "#         if isinstance(ids, torch.Tensor):\n",
    "#             test_ids.extend(ids.cpu().numpy().tolist())\n",
    "#         else:\n",
    "#             test_ids.extend(ids)\n",
    "\n",
    "# submission_df = pd.DataFrame({'id': test_ids, 'label': predictions})\n",
    "# submission_df['label'] = submission_df['label'].astype(int)\n",
    "# submission_df.to_csv('submission.csv', index=False)\n",
    "# print(\"Submission file created: submission.csv\")\n",
    "# # Save the model\n",
    "# torch.save(model.state_dict(), \"siamese_model.pth\")\n",
    "# print(\"Model saved as siamese_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25684f2",
   "metadata": {},
   "source": [
    "## Acknowledgments / AI Assistance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ee8d37",
   "metadata": {},
   "source": [
    "- The selection of the Siamese Neural Network comes from the google search lab. The query we put was 'what neural network structure possess the ability to compare two images?' And the answer was 'A Siamese Neural Network is a neural network structure specifically designed for comparing two images. It consists of two identical sub-networks, each processing one of the input images, and then the outputs of these sub-networks are compared to determine the similarity between the two images. '\n",
    "- We utilize some techniques like Squeeze-Excitation and residual block to improve our Siamese network's performance. We drawn the idea from online resources, notably articles by Paul-Louis Pr√∂ve and Neetu Sigger Choudhary on Medium: [Squeeze-and-Excitation Networks](https://medium.com/data-science/squeeze-and-excitation-networks-9ef5e71eacd7), [A Comprehensive Guide to Understanding and Implementing Bottleneck Residual Blocks](https://medium.com/@neetu.sigger/a-comprehensive-guide-to-understanding-and-implementing-bottleneck-residual-blocks-6b420706f66b).\n",
    "- We also ask Gemini to help refine and debug our SE block and residual block with prompt below:\n",
    "\n",
    "    class SEBlock(nn.Module):\n",
    "\n",
    "        def __init__(self, input_channels, reduction):\n",
    "            super(SEBlock, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_channels, input_channels // reduction)\n",
    "            self.fc2 = nn.Linear(input_channels // reduction, input_channels)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, x):\n",
    "            batch_size, channels, _, _ = x.size()\n",
    "            y = F.adaptive_avg_pool2d(x, 1).view(batch_size, channels)\n",
    "            y = F.relu(self.fc1(y))\n",
    "            yn = self.sigmoid(self.fc2(y)).view(batch_size, channels, 1, 1)\n",
    "            return x * y\n",
    "\n",
    "    class ResidualBlock(nn.Module):\n",
    "\n",
    "        def __init__(self, input_channels, reduction):\n",
    "            super(ResidualBlock, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(input_channels, input_channels, kernel_size=3, padding=1)\n",
    "            self.bn1 = nn.BatchNorm2d(input_channels)\n",
    "            self.se = squeeze_excite_block(input_channels, reduction)\n",
    "            self.conv2 = nn.Conv2d(input_channels, input_channels, kernel_size=3, padding=1)\n",
    "            self.bn2 = nn.BatchNorm2d(input_channels)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.conv1(x)\n",
    "            out = self.bn1(out)\n",
    "            out = F.relu(out)\n",
    "            out = self.se(out)\n",
    "            out = self.conv2(out)\n",
    "            out = self.bn2(out)\n",
    "            out += x\n",
    "            return F.relu(out)\n",
    "\n",
    "    help us refine the SEBlock and residual block according to the code file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
